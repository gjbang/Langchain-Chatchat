# Langchain-Chatchat 源代码部署/开发部署指南

## 0. 拉取项目代码

如果您是想要使用源码启动的用户，请直接拉取 master 分支代码

```shell
git clone https://github.com/chatchat-space/Langchain-Chatchat.git
```

## 1. 初始化开发环境

Langchain-Chatchat 自 0.3.0 版本起，为方便支持用户使用 pip 方式安装部署，以及为避免环境中依赖包版本冲突等问题，
在源代码/开发部署中不再继续使用 requirements.txt 管理项目依赖库，转为使用 Poetry 进行环境管理。

### 1.1 安装 Poetry

**由于包管理环境过于复杂，此处建议通过conda的虚拟环境进行安装，防止损坏本地Python环境**

> 在安装 Poetry 之前，如果您使用 Conda，请创建并激活一个新的 Conda 环境，例如使用 `conda create -n chatchat python=3.9` 创建一个新的 Conda 环境。

- `conda`在安装后需要进行环境设置，否则无法在命令行进行管理

```shell
# 初始化powershell
conda init powershell
# 调整powershell限制策略，管理员模式
Set-ExecutionPolicy -ExecutionPolicy RemoteSigned -Scope LocalMachine
```


安装 Poetry: [Poetry 安装文档](https://python-poetry.org/docs/#installing-with-pipx)

- 补充安装
  - 系统python环境安装 `pipx`，`py -m pip install --user pipx`，此脚本将`pipx`可执行程序安装到了Windows `Users`目录下，因此需在path增加`pipx`才可正常使用
  - `<USER folder>\AppData\Roaming\Python\Python3x\Scripts`，找到`pipx.exe`执行 `.\pipx.exe ensurepath`，添加系统路径
  - 更新 powershell 环境后，`pipx install poetry` 即可
  - `poetry`不能被安装在虚拟环境中，即既作为环境中的包，又作为对环境的管理者存在，因此在安装时将其安装外部系统环境中，并通过设置 `poetry config virtualenvs.prefer-active-python true` 优先处理当前 shell 中虚拟环境的安装部署请求


> [!Note]
> 如果你没有其它 poetry 进行环境/依赖管理的项目，利用 pipx 或 pip 都可以完成 poetry 的安装，

> [!Note]
> 如果您使用 Conda 或 Pyenv 作为您的环境/包管理器，在安装Poetry之后，
> 使用如下命令使 Poetry 使用 virtualenv python environment (`poetry config virtualenvs.prefer-active-python true`)

### 1.2 安装源代码/开发部署所需依赖库

进入主项目目录，并安装 Langchain-Chatchat 依赖

```shell
cd  Langchain-Chatchat/libs/chatchat-server/
poetry install --with lint,test -E xinference

# or use pip to install in editing mode:
pip install -e .
```

> [!Note]
> Poetry install 后会在你的虚拟环境中 site-packages 路径下生成一个 chatchat-`<version>`.dist-info 文件夹带有 direct_url.json 文件，这个文件指向你的开发环境

### 1.3 更新开发部署环境依赖库

当开发环境中所需的依赖库发生变化时，一般按照更新主项目目录(`Langchain-Chatchat/libs/chatchat-server/`)下的 pyproject.toml 再进行 poetry update 的顺序执行。

### 1.4 将更新后的代码打包测试

如果需要对开发环境中代码打包成 Python 库并进行测试，可在主项目目录执行以下命令：

```shell
poetry build
```

命令执行完成后，在主项目目录下会新增 `dist` 路径，其中存储了打包后的 Python 库。

## 2. 设置源代码根目录

如果您在开发时所使用的 IDE 需要指定项目源代码根目录，请将主项目目录(`Langchain-Chatchat/libs/chatchat-server/`)设置为源代码根目录。

执行以下命令之前，请先设置当前目录和项目数据目录：
```shell
cd Langchain-Chatchat/libs/chatchat-server/chatchat
export CHATCHAT_ROOT=/parth/to/chatchat_data
```

## 3. 关于 chatchat 配置项

从 `0.3.1` 版本开始，所有配置项改为 `yaml` 文件，具体参考 [Settings](settings.md)。

执行以下命令初始化项目配置文件和数据目录：
```shell
cd libs/chatchat-server
python chatchat/cli.py init
```

## 4. 初始化知识库

> [!WARNING]
> 这个命令会清空数据库、删除已有的配置文件，如果您有重要数据，请备份。

```shell
cd libs/chatchat-server
python chatchat/cli.py kb --recreate-vs
```
如需使用其它 Embedding 模型，或者重建特定的知识库，请查看 `python chatchat/cli.py kb --help` 了解更多的参数。

## 5. 启动服务

```shell
cd libs/chatchat-server
python chatchat/cli.py start -a
```

如需调用 API，请参考 [API 使用说明](api.md)


## 6. 模型配置

使用项目的默认模型推理和管理框架 Xinference，需要为该框架创建一个新的python环境

### 6.1 chatchat 接入配置

```shell
pip install xinference_client faiss-gpu  "unstructured[pdf]"
```

该行命令在配置chatchat相关包时已经安装


### 6.2 xinference环境

```shell
conda create -n xinference python=3.11
```
不必使用仓库的手册，且python版本也可以调整

```shell
# 需要cython进行编译
pip install cython
conda install -c conda-forge pynini==2.1.5
pip install "xinference[all]"
```
可以直接安装对应框架的所有内容

其中`llama-cpp-python`需要编译，安装 `visual studio`的c++桌面开发比较省事

`pynini==2.1.5`该包仅支持`linux`和`macos`，因为`windows`需要本地编译，而对编译器版本要求严格，`visual studio`或无法直接通过编译，因此使用`conda`已有的对应版本的包


```shell
pip install tiktoken  sentence-transformers
```
安装用于模型推理辅助的包

```shell
xinference-local
```
本地环境启动


### 6.3 Xinference内模型的加载

简单方法使用 `launch model`选择相关模型后，自动从 `huggingface` 等下载模型

对于自定模型则应当使用 `register model`从本地进行加载
- `model name` 不支持 `-` 等特殊字符，仅允许使用 `_`等常用字符
- `content length` 可以根据设备显存进行调整
- `model specs` 中的 `model size billions` 也需要根据实际情况进行调整


启动模型后，需要确定编码格式
`custome models - embedding models`，可以选择bge


### 6.4 chatchat配置模型

`v0.3`使用本地命令行进行配置，但是`v0.3.1`又改为使用`yaml`文件进行配置，可以直接进行修改，服务器无需重启即可自动更新

`python`执行前文的 `init` 指令后：

- 创建所有需要的数据目录
- 复制`samples` 知识库内容
- 生成默认 `yaml` 配置文件


**配置模型(model_settings.yaml)**：根据步骤 2. 模型推理框架并加载模型 中选用的模型推理框架与加载的模型进行模型接入配置，具体参考 model_settings.yaml 中的注释。主要修改以下内容：

```yaml
# 默认选用的 LLM 名称
 DEFAULT_LLM_MODEL: qwen1.5-chat

 # 默认选用的 Embedding 名称
 DEFAULT_EMBEDDING_MODEL: bge-large-zh-v1.5

# 将 `LLM_MODEL_CONFIG` 中 `llm_model, action_model` 的键改成对应的 LLM 模型
# 在 `MODEL_PLATFORMS` 中修改对应模型平台信息
```

**配置知识库路径(basic_settings.yaml)**：默认知识库位于 CHATCHAT_ROOT/data/knowledge_base，如果你想把知识库放在不同的位置，或者想连接现有的知识库，可以在这里修改对应目录即可。

```yaml
# 知识库默认存储路径
 KB_ROOT_PATH: D:\chatchat-test\data\knowledge_base

 # 数据库默认存储路径。如果使用sqlite，可以直接修改DB_ROOT_PATH；如果使用其它数据库，请直接修改SQLALCHEMY_DATABASE_URI。
 DB_ROOT_PATH: D:\chatchat-test\data\knowledge_base\info.db

 # 知识库信息数据库连接URI
 SQLALCHEMY_DATABASE_URI: sqlite:///D:\chatchat-test\data\knowledge_base\info.db
```



**配置知识库(kb_settings.yaml)**：默认使用 FAISS 知识库，如果想连接其它类型的知识库，可以修改 DEFAULT_VS_TYPE 和 kbs_config。

